{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import importlib\n",
    "import model\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import tqdm\n",
    "import kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(model)\n",
    "importlib.reload(kan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GRAMLayer\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 32),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LayerNorm(16),\n",
    "            nn.Linear(16, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class GRAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRAM, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            GRAMLayer(28 * 28, 32), GRAMLayer(32, 16), GRAMLayer(16, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.to(\"cuda\")\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(model, epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    train_losses = []  # List to store the training loss values\n",
    "    test_losses = []  # List to store the test loss values\n",
    "    accuracies = []  # List to store the test accuracies\n",
    "\n",
    "    for epoch in range(epochs):  # number of epochs\n",
    "        epoch_loss = 0\n",
    "        for images, labels in tqdm.tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm.tqdm(test_loader):\n",
    "                outputs = model(images)\n",
    "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "        accuracies.append(correct / total)\n",
    "\n",
    "        print(\n",
    "            \"Epoch: {}, Train Loss: {:.4f}, Test Loss: {:.4f}, Test Acc: {:.2f}\".format(\n",
    "                epoch, train_losses[-1], test_losses[-1], accuracies[-1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return train_losses, test_losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the GRAM model\n",
    "model = GRAM()\n",
    "gram_train_losses, gram_test_losses, gram_accuracies = train_and_test_model(model, 10)\n",
    "\n",
    "# Train the MLP model\n",
    "model = MLP()\n",
    "mlp_train_losses, mlp_test_losses, mlp_accuracies = train_and_test_model(model, 10)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(gram_train_losses, label=\"GRAM Train Loss\")\n",
    "plt.plot(gram_test_losses, label=\"GRAM Test Loss\")\n",
    "plt.plot(mlp_train_losses, label=\"MLP Train Loss\")\n",
    "plt.plot(mlp_test_losses, label=\"MLP Test Loss\")\n",
    "plt.title(\"Model Convergence\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(gram_accuracies, label=\"GRAM Test Acc\")\n",
    "plt.plot(mlp_accuracies, label=\"MLP Test Acc\")\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
